{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import html\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from random import randint\n",
    "from itertools import chain\n",
    "from math import log\n",
    "\n",
    "from numpy import zeros\n",
    "from numpy.linalg import svd\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "from gensim.models.word2vec import LineSentence, Word2Vec\n",
    "\n",
    "from sklearn.utils.extmath import randomized_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "def browser_alert(message):\n",
    "    display(HTML('<script type=\"text/javascript\">alert(\"' + message + '\");</script>'))\n",
    "    \n",
    "def browser_notify(message):\n",
    "    display(HTML('<script type=\"text/javascript\">var notification=new Notification(\"Jupyter Notification\",{icon:\"http://blog.jupyter.org/content/images/2015/02/jupyter-sq-text.png\",body:\"' + message + '\"});</script>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input file cleaning and formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_str(string):  \n",
    "    string = html.unescape(string)\n",
    "    string = re.sub(r\"[^A-Za-z0-9!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"[ ]+\", \" \", string)\n",
    "\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentences_from_line(line):\n",
    "    \n",
    "    sentences = list()\n",
    "    if line[0:2] == \"--\" or len(line.strip()) == 0:\n",
    "        return sentences\n",
    "    \n",
    "    sentences.extend(sent_tokenize(line.strip()))\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_article_sentences(article_file_path):\n",
    "    with open(article_file_path) as article_file:\n",
    "        for line in article_file:\n",
    "            sentences = get_sentences_from_line(line)\n",
    "            \n",
    "            for sentence in sentences:\n",
    "                yield(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sample_article_path = \"/home/v2john/financial-news-dataset/20061020_20131126_bloomberg_news/2009-01-02/rust-through-transparency\"\n",
    "# for article_sentence in get_article_sentences(sample_article_path):\n",
    "#     print(clean_str(article_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpora_path = \"/home/v2john/financial-news-dataset/20061020_20131126_bloomberg_news/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "consolidated_output_path = \"/home/v2john/financial-news-dataset/bloomberg_sentences.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(consolidated_output_path, 'w') as consolidated_output_file:\n",
    "    for path, subdirs, files in os.walk(corpora_path):\n",
    "        for name in files:\n",
    "            full_path = os.path.join(path, name)\n",
    "            if \"/.\" not in full_path:\n",
    "                try:\n",
    "                    for article_sentence in get_article_sentences(full_path):\n",
    "                        consolidated_output_file.write(clean_str(article_sentence) + \"\\n\")\n",
    "                except Exception as e:\n",
    "                    print(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "browser_notify(\"Sentences generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Learning Word2Vec word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reuters_sentences = LineSentence('/home/v2john/Projects/financial-news-dataset/reuters_sentences.txt')\n",
    "bloomberg_sentences = LineSentence('/home/v2john/Projects/financial-news-dataset/bloomberg_sentences.txt')\n",
    "w2v_model_path = \"/home/v2john/Projects/financial-word-embedder/models/w2v_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(chain(reuters_sentences, bloomberg_sentences), size=400, window=5, min_count=25, workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "# model.save(w2v_model_path)\n",
    "\n",
    "# restore model\n",
    "model = Word2Vec.load(w2v_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "browser_notify(\"Word embeddings training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.wv['shareholder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.similar_by_word(\"stock\", topn=10, restrict_vocab=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_graph = dict()\n",
    "k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for word in model.wv.vocab.keys():\n",
    "    similar_word_tuples = model.similar_by_word(word, topn=k, restrict_vocab=None)\n",
    "    similar_words = list()\n",
    "    for similar_word_tuple in similar_word_tuples:\n",
    "        similar_words.append(similar_word_tuple[0])\n",
    "        \n",
    "    word_graph[word] = similar_words\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "browser_notify(\"Word graph created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(word_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# seed_words = ['shrink', 'drop', 'fall', 'plunge', 'slump']\n",
    "seed_words = ['surge', 'rise', 'jump', 'gain']\n",
    "walk_dict = dict()\n",
    "random_walk_length = 100\n",
    "walk_iterations = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perform_random_walk(word_graph, word, iterations, walk_dict):\n",
    "    words_chosen = list()\n",
    "    for i in range(iterations):\n",
    "        word_choices = word_graph[word]\n",
    "        word_choice = word_choices[randint(0, k-1)]\n",
    "        \n",
    "        words_chosen.append(word_choice)\n",
    "        word = word_choice\n",
    "    \n",
    "    for word_choice in words_chosen:\n",
    "        if word_choice in walk_dict.keys():\n",
    "            walk_dict[word_choice] += 1\n",
    "        else:\n",
    "            walk_dict[word_choice] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for word in seed_words:\n",
    "    for iteration in range(walk_iterations):\n",
    "        perform_random_walk(word_graph, word, random_walk_length, walk_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted(walk_dict, key=walk_dict.get, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "browser_notify(\"Words identified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# walk_dict[\"zealand's\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn SVD Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences_path = \"/home/v2john/financial-news-dataset/all_sentences.txt\"\n",
    "context_width = 2\n",
    "vocabulary = set()\n",
    "corpus_term_frequency = 0\n",
    "term_frequencies = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(sentences_path) as sentences_file:\n",
    "    for line in sentences_file:\n",
    "        tokens = word_tokenize(line)\n",
    "        num_tokens = len(tokens)\n",
    "        \n",
    "        for i in range(num_tokens):\n",
    "            if tokens[i] not in term_frequencies:\n",
    "                term_frequencies[tokens[i]] = 1\n",
    "            else:\n",
    "                term_frequencies[tokens[i]] += 1\n",
    "        \n",
    "        corpus_term_frequency += num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary = {k for k,v in term_frequencies.items() if v >= 100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(corpus_term_frequency)\n",
    "print(len(vocabulary))\n",
    "# print(term_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "browser_notify(\"Corpus built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "npmi_matrix = zeros((len(vocabulary), len(vocabulary)))\n",
    "vocab_list = list(vocabulary)\n",
    "print(len(vocab_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_pos_dict = dict()\n",
    "counter = 0\n",
    "\n",
    "for word in vocab_list:\n",
    "    vocab_pos_dict[word] = counter\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(sentences_path) as sentences_file:\n",
    "    for line in sentences_file:\n",
    "        tokens = word_tokenize(line)\n",
    "        num_tokens = len(tokens)\n",
    "        \n",
    "        for i in range(num_tokens):\n",
    "            current_word = tokens[i]\n",
    "            try:\n",
    "                for j in range(i, i + context_width + 1):\n",
    "                    context_word = tokens[j]\n",
    "                    if current_word in vocabulary and context_word in vocabulary:\n",
    "                        npmi_matrix[vocab_pos_dict[current_word]][vocab_pos_dict[context_word]] += 1 \n",
    "                        npmi_matrix[vocab_pos_dict[context_word]][vocab_pos_dict[current_word]] += 1 \n",
    "            except IndexError as ie:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(vocabulary)):\n",
    "    for j in range(len(vocabulary)):\n",
    "        joint_prob = npmi_matrix[i][j] / corpus_term_frequency\n",
    "        term1_prob = term_frequencies[vocab_list[i]] / corpus_term_frequency\n",
    "        term2_prob = term_frequencies[vocab_list[j]] / corpus_term_frequency\n",
    "        \n",
    "        if joint_prob > 0 and term1_prob > 0 and term2_prob > 0:\n",
    "            mutual_information = \\\n",
    "                log(joint_prob / (term1_prob * term2_prob)) / -log(joint_prob)\n",
    "        else:\n",
    "            mutual_information = 0.0\n",
    "            \n",
    "        npmi_matrix[i][j] = mutual_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "u, sigma, vt = randomized_svd(npmi_matrix, n_components=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning related words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svd_word_embeddings_path = \"/home/v2john/svd_word_embeddings.pkl\"\n",
    "vocab_path = \"/home/v2john/financial_vocab.pkl\"\n",
    "similarity_dict_path = \"/home/v2john/similarity_dict.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Save vectors\n",
    "# with open(svd_word_embeddings_path, 'wb') as svd_word_embeddings_file:\n",
    "#     pickle.dump(u, svd_word_embeddings_file)\n",
    "    \n",
    "# with open(vocab_path, 'wb') as vocab_file:\n",
    "#     pickle.dump(vocab_list, vocab_file)\n",
    "\n",
    "# browser_notify(\"Persisted to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Restore\n",
    "# with open(svd_word_embeddings_path, 'rb') as svd_word_embeddings_file:\n",
    "#     u = pickle.load(svd_word_embeddings_file)\n",
    "    \n",
    "# with open(vocab_path, 'rb') as vocab_file:\n",
    "#     vocab_list = pickle.load(vocab_file)\n",
    "\n",
    "# browser_notify(\"Persisted to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "browser_notify(\"Embeddings learnt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 10\n",
    "global_similarity_dict = dict()\n",
    "num_words = len(vocab_list)\n",
    "\n",
    "for i in range(num_words):\n",
    "    print(\"Word \" + str(i + 1) + \" of \" + str(num_words))\n",
    "    word_similarity_dict = dict()\n",
    "    for j in range(num_words):\n",
    "        if j == i:\n",
    "            continue    \n",
    "        word_similarity_dict[vocab_list[j]] = cosine(u[i], u[j])\n",
    "    \n",
    "    global_similarity_dict[vocab_list[i]] = \\\n",
    "        list(map(lambda x: x[0], sorted(word_similarity_dict.items(), key=lambda x: x[1], reverse=True)[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Save vectors\n",
    "\n",
    "# with open(similarity_dict_path, 'wb') as similarity_dict_file:\n",
    "#     pickle.dump(global_similarity_dict, similarity_dict_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# seed_words = ['shrink', 'drop', 'fall', 'plunge', 'slump']\n",
    "# seed_words = ['surge', 'rise', 'jump', 'gain']\n",
    "seed_words = ['random', 'garbage']\n",
    "# seed_words = ['dire']\n",
    "walk_dict = dict()\n",
    "random_walk_length = 1000\n",
    "walk_iterations = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perform_random_walk(word_graph, word, iterations, walk_dict):\n",
    "    words_chosen = list()\n",
    "    for i in range(iterations):\n",
    "        word_choices = word_graph[word]\n",
    "        word_choice = word_choices[randint(0, k-1)]\n",
    "        \n",
    "        words_chosen.append(word_choice)\n",
    "        word = word_choice\n",
    "    \n",
    "    for word_choice in words_chosen:\n",
    "        if word_choice in walk_dict.keys():\n",
    "            walk_dict[word_choice] += 1\n",
    "        else:\n",
    "            walk_dict[word_choice] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for word in seed_words:\n",
    "    for iteration in range(walk_iterations):\n",
    "        perform_random_walk(global_similarity_dict, word, random_walk_length, walk_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(walk_dict, key=walk_dict.get, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
